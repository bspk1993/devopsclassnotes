Docker Installation
-------------------

-- yum install docker -y

-- systemctl start docker

-- systemctl status docker

-- docker -v


Commands
--------

-- docker images        [ To show list of images ]

-- docker ps             [To Show list of Containers]

There are two ways to get images

   1. Create image from Dockerfile

   2. Pull the images from dockerhub


-- docker pull amazonlinux    [ To get image amazonlinux or ubuntu from dockerhub ]

-- docker pull ubuntu           [ To get image ubuntu from dockerhub ]

-- docker images

A Docker image is made up of multiple layers, which are stacked on top of each other. Each layer represents a set of file changes (like adding, modifying, or deleting files) and helps in optimizing storage and reusability.


Now, lets try creating a container using this images. Below command will directly login to contains as we are using -it

-- docker run -it --name amazonlinuxcontainer amazonlinux   [ To create container , -it = interactive(go inside container), amazonlinuxcontainer is a container name , amazonlinux is image ]
      -- ls
      -- yum install git maven tree httpd -y
      -- check the version
      -- touch file{1..10}

      -- ctrl p q   [ To exit from container]

If you dont want to login to container and just want to create a container and run in detach mode use -d

-- docker run -it -d --name ubuntucontainer ubuntu

-- docker ps -a  [To list all running and stopped containers]

-- docker ps    [ To list only running containers ]

If you want to access the container

-- docker attach ubuntucontainer
   apt update
   apt install nginx -y
 
      -- ctrl p q

If you want to run direct commands on container use exec

-- docker exec -it 1275f407dc04 /bin/bash

        -- apt update -y
        -- apt install git -y
        -- apt install apache2 -y
        -- service apache2 start
        -- service apache2 status

        -- ctrl p q

Docker attach and exec will help to access the container

-- docker attach 1275f407dc04

       -- ctrl p q

-- docker ps -a

docker attach
-------------
is used to attach your terminal to the standard input, output, and error streams of the main process running inside a container.
It allows you to interact with the container's main process as if you were running it directly in your terminal.

docker exec
-----------

is used to run a new command in an already running container.
This command allows you to start additional processes inside the container independently of the main process.

docker attach interacts with the container's main process (PID 1).
docker exec starts a new, separate process inside the container.

EXEC is preferred

Summary
-------

docker images
docker pull ubuntu
docker run -it --name ubuntucontainer ubuntu
docker pull amazonlinux
docker run -it -d --name amazoncontainer amazonlinux
docker exec -it 1275f407dc04 /bin/bash
docker attach 1275f407dc04

Few container commands
----------------------

docker search ubuntu        : to search images
docker ps -a            : to list all containers
docker ps             : to list runnings containers
docker stop testcontainer    : to stop the container , it will be in exit state
docker start testcontainer    : to start the container
docker pause testcontainer    : to pause the container,it will be in pause state. you cannot connect to this container,
          try docker exec -it contid /bin/bash
docker unpause cont_name        : to unpause container
docker attach testcontainer    : to go inside the container
docker inspect testcontainer    : to get the complete info container
docker kill testcontainer       : to kill the container or forcibly stop the container
docker rm testcontainer         : TO delete the container

docker stop containerid/containername

STOP: will wait to finish all process running inside container
KILL: wont wait to finish all process running inside container

=============================================================================================================

TASK: Pull ubuntu image and install apache and mysql server software's
------------

docker images
docker pull ubuntu
docker run -it --name ubuntucontainer ubuntu
  -- apt update -y
  -- apt install apache2 python3 mysql-server -y
  -- ctrl p q

Now we have container with name ubuntucontainer with apache2 and mysql server, lets create our image with this container ubuntucontainer
This command is used to create a image what ever data we have in the container

-- docker commit ubuntucontainer myimagewithapachenmysql

-- docker images

If you want to create a new container with this image myimagewithapachenmysql

-- docker run -it -d --name mycontwithapachemysql myimagewithapachenmysql

-- docker attach mycontwithapachemysql          [Access the container and check the version]
        -- mysql --version

ctrl pq --> after exit, container is still running
ctrl d --> container will  be stopped if you use ctl d

docker ps -a  [See container is in exit state]
docker start cont-name
docker ps -a

What we did?

Ubuntu image from dockerhub -> create a container -> Take image(custom image) -> From this images create containers again
                                 install softwares     this image
                               contains software's
manually downloaded  --> Manually created container and installed

This is not a good practise , this is manual

Clean up
---------

docker ps -a

Delete all containers
----------------------

docker ps -aq  [List only container-ids]

docker kill $(docker ps -aq)  

docker ps -a   [ All containers are in exit state ]

docker rm $(docker ps -aq)

docker ps -a

Delete all the images
---------------------

docker images -q

docker rmi -f $(docker images -q)

docker images

===========================
DAY 2
===========================================================================================================================================

If you want to automate all these use Dockerfile

DOCKERFILE
----------

It is used to automate image creation.
Inside Dockerfile we use components to do our works.
Components will be on Capital Letter.
In Dockerfile D will be capital.
We can create image directly without container help
To create image from Dockerfile we need to build it.

COMPONENTS:
----------

FROM        : used to get base image
RUN        : used to run linux commands (During image creation)
CMD        : used to run linux commands (After container creation)
ENTRYPOINT    : high priority than cmd
COPY        : to copy local files to container
ADD        : to copy internet files to container
WORKDIR        : to open req directory
LABEL        : to add labels for docker images
ENV        : to set env variables (inside container)
ARGS        : to pass env variables (outside containers)
EXPOSE        : to give port number

Dockerfile --> Docker Build -->  Image --> Container

docker rm -f [container]       : Forcefully remove even if it is running
docker logs [container]        : View logs for a running container:
docker top  [container]        : Show running processes in a container:
docker stats [container]       : View live resource usage statistics for a container:
docker diff [container]        : Show changes to files or directories on the filesystem:

docker cp [file-path] CONTAINER:[path] : Copy a local file to a directory in a container:
         
             docker cp hello.txt cont1:/tmp

docker save IMAGE > IMAGE.tar   : Save an Image to tar file

             docker save ubuntu > ubuntu.tar

docker load -i IMAGE.tar    : Load an image from tar file

              docker load -i ubuntu.tar

docker history IMAGE        : Shows image history

docker image prune        : Delete unused images

Example 1
=========
vi Dockerfile

FROM ubuntu
RUN apt update -y
RUN apt install git maven tree apache2 -y
RUN touch file1

-- docker build -t reyaz:v1 .    [  . represent current directory where we have dockerfile ]

-- docker run -it --name cont1 reyaz:v1   [ You will be now in container and see versions of softwares installed ]

   -- ctrl pq

Example 2
===========
RUN will execute while image creation
CMD will exeute after container creation

vi Dockerfile

FROM ubuntu
RUN apt update -y
RUN apt install git maven tree apache2 -y
RUN touch file1
RUN apt install python3 -y
CMD apt install mysql-server -y

Note already first few lines are execute, it will not execute again, it will run last lines

CMD will not executed, because it will execute after container creation

-- docker build -t reyaz:v2 .

-- docker run -it --name cont2 reyaz:v2   [dont use -d here for now to see the installation]

--> now mysql-server is now installing because we used CMD command


Example 3
========
COPY --> copy local files to container
ADD --> copy internet files to container

touch index.html

vi Dockerfile

FROM ubuntu
COPY index.html /tmp
ADD https://dlcdn.apache.org/tomcat/tomcat-10/v10.1.35/bin/apache-tomcat-10.1.35.tar.gz /tmp


-- docker build -t reyaz:v3 .
-- docker run -it --name cont3 reyaz:v3

now you are in container
cd /tmp
ls


Example 4
========
WORKDIR --> by default , when you are in container it will be in /, if you want to have a default path WORKDIR will use
LABEL --> just like a tag , we are labeling

vi Dockerfile

FROM ubuntu
COPY index.html /tmp
ADD https://dlcdn.apache.org/tomcat/tomcat-10/v10.1.35/bin/apache-tomcat-10.1.35.tar.gz /tmp
WORKDIR /tmp
LABEL author Reyaz


-- docker build -t reyaz:v4 .
-- docker run -it --name cont4 reyaz:v4

--> now see the path is /tmp
--> to see the entire info about container

-- docker inspect cont4
-- docker inspect cont4 | grep Reyaz



Example 5
==========

vi Dockerfile

FROM ubuntu
ENV course devops
ENV trainer Reyaz
EXPOSE 8080

docker build -t reyaz:v5 .
docker run -it --name cont5 reyaz:v5

echo $course
echo $trainer

ctrl pq

docker ps -a  [ Now you can see the port number for cont5 ]


First Dockerfile Task for application Deployment
===============================================

Code - GitHub --> DockerFile --> Build Image --> Create Container --> Access application

yum install git -y

git clone https://github.com/ReyazShaik/website.git

--> dont go inside website folder, create Dockerfile outside website

vi Dockerfile

FROM ubuntu
RUN apt update
RUN apt install apache2 -y
RUN apt install apache2-utils -y
RUN apt clean
COPY website/ /var/www/html/
RUN service apache2 restart
EXPOSE 80
CMD ["/usr/sbin/apachectl", "-D", "FOREGROUND"]


-D FOREGROUND â†’ Runs Apache in the foreground mode, meaning:
Apache does not daemonize (does not run in the background).
Logs are printed to the container's stdout/stderr, making it visible with docker logs.
The process stays alive, preventing the container from exiting immediately.
Containers stop when the main process exits. Running Apache in foreground mode ensures the container remains active.


-- docker build -t firstproject:v1 .

-- docker run -itd --name newwebcont1 -p 80:80 firstproject:v1

In AWS SG , allow All traffic

http://ip

Note: docker rmi firstimage:v1 [if required]

Another example
----------------
Tomcat installation on Container
-------------------------------

vi Dockerfile

# Use the official Tomcat image as the base
FROM tomcat:latest

# Set environment variables (optional)
ENV CATALINA_HOME /usr/local/tomcat
ENV PATH $CATALINA_HOME/bin:$PATH

# Remove default webapps (optional: keeps Tomcat clean)
RUN rm -rf $CATALINA_HOME/webapps/*

# Copy your application WAR file into the Tomcat webapps directory
COPY jenkins.war $CATALINA_HOME/webapps/jenkins.war

# Expose Tomcat's default HTTP port
EXPOSE 8080

# Start Tomcat
CMD ["catalina.sh", "run"]


-- docker build -t tomcatimage:v1 .

-- docker run -itd --name tomcont -p 8080:8080 tomcatimage:v1
                                     host:container

docker exec -it 1275f407dc04 /bin/bash

http://ip:8080

======================================================================================================================


VOLUME:
--------
Docker volumes are a way to persist data generated by and used by Docker containers.

Volumes are stored on the host filesystem outside of the container's filesystem, which means they are not deleted when the container stops.

This makes them ideal for managing persistent data.

In Short
-------

If you want to store the data in container use volumes
volume is just a folder in container
containers use host resources (cpu, ram, )
volume can be shared to multiple containers.
At a time we can share single volume.
data inside volume will store on local.

Method 1 -- creating volume from Dockerfile
----------------------------------------

vi Dockerfile

FROM ubuntu
VOLUME ["volume1"]


-- docker build -t reyaz:v1 .

-- docker run -it --name cont1 reyaz:v1

ls --> you will be in container automatically and do ls -- you can see volume1

cd volume1

touch file{1..5}

--> what ever data you have created in container in volume1 the same data will be in docker host and vice versa
it is bidirectional -- if you create data in docker host it will be available in container also

comeout of the container ctrl pq and go to the below path

/var/lib/docker/volumes -- is the place where volume data is stored

touch python{1..5}  --> create some sample files, it should be visible inside container

-- docker attach cont1

if stopped docker start cont1

docker attach cont1

cd volume1

ls

------------------ SHARE THE VOLUME ------------------

If you want to get the same volume to another container

-- docker run -it --name cont2 --volumes-from cont1 ubuntu

Optional
docker run -it --name cont2 --volumes-from cont1 --privileged=true  ubuntu

ls

cd volume1


come out of the container ctrl pq

docker ps -a

--> Ex: create a new container with same volume : just an example

docker run -it --name cont3 --volumes-from cont2 ubuntu


Method 2 - Creating volume from the command directly
-------

docker images

docker run -it --name cont4 -v /volume2 ubuntu

cd volume2/

touch python{1..5}

ctrl pq

--> now share the volume from cont4 to cont5

docker run -it --name cont5 --volumes-from cont4 ubuntu
ls
cd volume2
now you can see the same data
create files here , we can see the same data in cont4 also
touch python{6..10}

ctrl pq

docker attach cont4  --> connect to cont4 to see the data , same data available
ls

--> you can see the same data also in docker host /var/lib/docker/volumes/



Method 3 -- Volume Mounting  
--------
Now you are in docker host

docker volume ls

docker volume create volume3

docker volume inspect volume3

cd /var/lib/docker/volumes/volume3/_data  -- in docker host

touch html{1..5}

docker run -it --name cont6 --mount source=volume3,destination=/volume3 ubuntu

now you are in container 6 , cd /volume3 and ls

now create files here , it will be available on docker host

if you delete, everywhere it is deleted


4. SHARING LOCAL FILES to Container
----------------------------------

be in docker host
touch reyaz{1..10}
docker volume inspect volume3
cp * /var/lib/docker/volumes/volume3/_data
docker attach cont6
cd volume3


Use AWS EBS Volumes to docker containers
---------------------------------------

Create 20GB EBS volume in AWS Console

Attach to the EC2 instance as /dev/xvdb

lsblk          -- to list the file sytem
sudo mkfs -t ext4 /dev/xvdb        ----- format the disk before use
sudo mkdir /mnt/ebs-volume
sudo mount /dev/xvdb /mnt/ebs-volume
docker images
docker run -it  --name my-container1 -v /mnt/ebs-volume:/mynewebsvolume ubuntu
  -- ls
  -- cd mynewebsvolume
  -- touch hello

-- create a another container with new name from before container

docker run -it --name mynewcont2 --volumes-from my-container1  ubuntu

  -- ls
  -- cd mynewebsvolume

you see the same data in all containers



DOCKER SYSTEM COMMANDS:
----------------------
to know the docker components resource utilization
docker system df
docker system df -v

docker ps -a

docker inspect cont5

docker inspect cont5 | grep volume -i

RESOURCE MANAGEMENT:
--------------------
By default, docker containers will not have any limits for the resources like cpu ram and memory so we need to restrict resource use for container.

By default docker containers will use host resources(cpu, ram, rom)
Resource limits of docker container should not exceed the docker host limits.

docker stats  --> to check live cpu and memory

docker run -it --name cont7 --cpus="0.1" --memory="300mb" ubuntu
docker update cont7 --cpus="0.7" --memory="300mb"

JENKINS SETUP BY DOCKER:
docker run -it -d --name jenkins -p 8080:8080 jenkins/jenkins
-- docker exec -it jenkins /bin/bash


======== Delete all container ============
docker kill $(docker ps -aq)  

docker rm $(docker ps -aq)

docker ps -a

======== Delete images =====================
docker rmi -f $(docker images -q)

======================================================
Docker Compose
======================================================

Before docker compose how we use to do?



Lets Use HDFC Bank example : internetbanking, MobileBanking, Insurance and Loans

We need to create for all modules a separate containers with some webserver to run, for this write Dockerfile

Create a directory called internetbanking , MobileBanking, Insurance and Loans

In Every directory have index.html and Dockerfile.

Create a image and container with that index.html

how?

yum install -y git

git clone https://github.com/ReyazShaik/hdfcwebsite.git

-- cd hdfcwebsite

-- cd internetbanking

Here already index.html is available, just create a Dockerfile

vi Dockerfile

FROM ubuntu
RUN apt update -y
RUN apt install apache2 -y
COPY index.html /var/www/html
CMD ["/usr/sbin/apachectl", "-D", "FOREGROUND"]

-- docker build -t internetbanking:v1 .

-- docker run -itd --name cont1 -p 81:80 internetbanking:v1

-- docker ps -a

http://instanceip:81

-- docker logs cont1 [If required]

-----------

MobileBanking
------------

cd ..

cd mobilebanking

vi Dockerfile


FROM ubuntu
RUN apt update -y
RUN apt install apache2 -y
COPY index.html /var/www/html
CMD ["/usr/sbin/apachectl", "-D", "FOREGROUND"]


-- docker build -t mobilebanking:v1 .

-- docker run -itd --name cont2 -p 82:80 mobilebanking:v1

http://instanceip:82

------------

Insurance
---------
cd ..
cd insurance

vi Dockerfile


FROM ubuntu
RUN apt update -y
RUN apt install apache2 -y
COPY index.html /var/www/html
CMD ["/usr/sbin/apachectl", "-D", "FOREGROUND"]


docker build -t insurance:v1 .

docker run -itd --name cont3 -p 83:80 insurance:v1

http://instanceip:83

docker ps -a

docker images
--------------------------------------------------

Loans
---------
cd ..
cd loans

vi Dockerfile


FROM ubuntu
RUN apt update -y
RUN apt install apache2 -y
COPY index.html /var/www/html
CMD ["/usr/sbin/apachectl", "-D", "FOREGROUND"]


docker build -t loan:v1 .

docker run -itd --name cont4 -p 83:80 loan:v1

http://instanceip:83

docker ps -a

docker images


What ever we did is not good process and not used in realtime - use docker compose


Docker Compose
-------------

Docker Compose is a tool that allows you to define and run multi-container Docker applications using a YAML file (docker-compose.yml). Instead of running multiple docker run commands manually, you can define everything in one file and start your services with a single command.

Docker Compose is a tool for defining and running multi-container Docker applications.

With Docker Compose, you can use a YAML file to define the services, networks, and volumes required for your application, and then bring up the entire stack with a single command.

It is a tool used to create multiple containers.

It will work on single host.

we can create, stop, start and delete all containers together

we can write a file called docker-compose which will be on yaml format.

Docker Compose Installation
---------------------------

vi dockercompose.sh

sudo curl -L "https://github.com/docker/compose/releases/download/1.29.1/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
ls /usr/local/bin/
sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose
docker-compose version

sh dockercompose.sh


First, lets kill all containers

docker kill $(docker ps -a -q)

docker rm $(docker ps -a -q)


vi docker-compose.yml

version: '3.8'
services:
  internetbanking:
    image: internetbanking:v1
    ports:
      - "81:80"
  mobilebanking:
    image: mobilebanking:v1
    ports:
      - "82:80"
  insurance:
    image: insurance:v1
    ports:
      - "83:80"
  loan:
    image: loans:v1
    ports:
      - "84:80"

docker ps -a ---> No containers

docker-compose up -d  --> detach mode

docker-compose ps

refresh http://IP:81 and all in browser

docker-compose stop
docker-compose start
docker-compose kill -- to stop, use stop or kill
docker-compose start
docker-compose

TO remove containers first kill and rm

docker-compose kill
docker-compose rm

To create container again

docker-compose up -d
docker-compose ps
docker-compose pause
docker-compose unpause

docker-compose logs

docker-compose images --> these images are managed by docker-images , docker images shows managed by docker

docker-compose top

docker-compose restart

docker-compose scale loan=10  --> it will create but port conflict will come , if you want to scale we have dockerswarm

docker-compose down --> it will stop and kill automatically


CHANGING THE DEFULT FILE:

by default the docker-compose will support the following names
docker-compose.yml, docker-compose.yaml, compose.yml, compose.yaml

mv docker-compose.yml reyaz.yml
docker-compose up -d    : throws an error

docker-compose -f reyaz.yml up -d
docker-compose -f reyaz.yml ps
docker-compose -f reyaz.yml down

Load Balancing Project with Docker-Compose
==========================================
ðŸ“Œ Nginx Load Balancer - Dockerfile Setup
--------------------------------------


                                   --- Container1- backend1
User --------> NGinx Load Balancer --- Container2- backend2
                                   --- Container3- backend3


This Dockerfile will set up Nginx as a Load Balancer for multiple backend services inside Docker.

Step 1: Create a Dockerfile
Step 2: Create nginx.conf locally and copy to containers through docker-compose
Step 3: Create docker-compose file to create containers in one shot

1ï¸âƒ£ Create the Dockerfile
---------------------

vi Dockerfile
# Use the official Nginx image
FROM nginx:latest

# Remove default config and copy custom nginx.conf
RUN rm /etc/nginx/conf.d/default.conf
COPY nginx.conf /etc/nginx/conf.d/

# Expose port 80
EXPOSE 80

# Start Nginx
CMD ["nginx", "-g", "daemon off;"]



is used to keep Nginx running in the foreground when running inside a Docker container.
ðŸ”¹ Why is daemon off; Needed?
By default, Nginx runs as a background (daemon) process.
In a Docker container, the main process must stay in the foreground.
If Nginx runs as a daemon, Docker thinks the container has exited.
Setting "daemon off;" prevents it from running in the background, keeping the container alive



2ï¸âƒ£ Create the Nginx Load Balancer Config (nginx.conf)
------------------------------------------

vi nginx.conf

# Define the upstream backend servers
upstream backend {
    server backend1:5000;
    server backend2:5001;
    server backend3:5002;
}

server {
    listen 80;

    location / {
        proxy_pass http://backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}

3ï¸âƒ£ Create a docker-compose.yml (Optional)
-------------------------------------

vi docker-compose.yml

version: '3'
services:
  nginx:
    build: .
    container_name: nginx-lb
    ports:
      - "80:80"
    depends_on:
      - backend1
      - backend2
      - backend3

  backend1:
    image: ghcr.io/benc-uk/python-demoapp
    container_name: backend1
    expose:
      - "5000"

  backend2:
    image: ghcr.io/benc-uk/python-demoapp
    container_name: backend2
    expose:
      - "5001"

  backend3:
    image: ghcr.io/benc-uk/python-demoapp
    container_name: backend3
    expose:
      - "5002"


docker-compose up -d
docker-compose ps

(or)

docker build -t nginx-lb .
docker run -d -p 80:80 --name nginx-lb nginx-lb


âœ… Explanation:

Nginx Load Balancer (nginx-lb) forwards traffic to backend1, backend2, and backend3.
Each backend service listens on different ports.

================================================================
ðŸ“Œ Docker Load Balancing with Python Application using Nginx
===============================================================

âœ… Deploy multiple Python Flask applications as backend services.
âœ… Use Nginx as a load balancer to distribute traffic.
âœ… Use Docker Compose for easy deployment.


1ï¸âƒ£ Create the Python Flask App (app.py)
==================================
vi app.py

from flask import Flask
import socket

app = Flask(__name__)

@app.route('/')
def hello():
    return f"Hello from {socket.gethostname()}!"

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)

2ï¸âƒ£ Create requirements.txt
====================

vi requirements.txt
flask

3ï¸âƒ£ Create the Python App Dockerfile
==============================

vi Dockerfile

# Use Python base image
FROM python:3.9

# Set working directory
WORKDIR /app

# Copy and install dependencies
COPY requirements.txt .
RUN pip install -r requirements.txt

# Copy the application code
COPY . .

# Expose the port Flask runs on
EXPOSE 5000

# Start the Flask app
CMD ["python", "app.py"]


4ï¸âƒ£ Create Nginx Load Balancer Config (nginx.conf)
=======================================
vi nginx.conf

# Define the upstream backend servers
upstream backend {
    server backend1:5000;
    server backend2:5000;
    server backend3:5000;
}

server {
    listen 80;

    location / {
        proxy_pass http://backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}


5ï¸âƒ£ Create docker-compose.yml
=========================

vi docker-compose.yml

version: '3'

services:
  nginx:
    image: nginx:latest
    container_name: nginx-lb
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/conf.d/default.conf
    depends_on:
      - backend1
      - backend2
      - backend3

  backend1:
    build: .
    container_name: backend1
    expose:
      - "5000"

  backend2:
    build: .
    container_name: backend2
    expose:
      - "5000"

  backend3:
    build: .
    container_name: backend3
    expose:
      - "5000"


docker-compose up -d --build

docker ps -a

http://IP



=======================
DOCKERHUB
=======================


docker images

instead storing images in local docker host , push to dockerhub

go to docker and signup

[Lets first tag our image]

-- docker tag internetbanking:v1 trainerreyaz/ib-image

-- docker push trainerreyaz/ib-image --> it will fail, no authentication

-- docker login

    username:
    password:

-- docker push trainerreyaz/ib-image

**** Now see the images in dockerhub
*** One time docker login is enough

docker tag mobilebanking:v1 trainerreyaz/mb-image
docker push trainerreyaz/mb-image

docker tag insurance:v1 trainerreyaz/insurance-image
docker push trainerreyaz/insurance-image

docker tag loans:v1 trainerreyaz/loans-image
docker push trainerreyaz/loans-image

--- Now all your images are in dockerhub, just delete the images from local docker host and you can get them from dockerhub again

-- docker rmi -f $(docker images -aq)

-- docker pull trainerreyaz/ib-image
-- docker pull trainerreyaz/mb-image
-- docker pull trainerreyaz/insurance-image
-- docker pull trainerreyaz/loans-image

Now pull images from dockerhub

-- docker pull trainerreyaz/ib-image:latest

-- docker run -itd --name cont1 -p 80:80 trainerreyaz/ib-image:latest

=============
DOCKER SWARM - High Availability
=============

If container deleted we can re-create manually or using docker-compose, by if the docker host is terminated ?

Docker swarm uses multiple host machines

Its an orchestration tool for containers.
It is cluster used to manage containers.
Cluster means group of servers.
Cluster will have manager/Leader and worker nodes.
Multiple servers will have same container.
If we can access container from one server we can access from another server.
Manager node will distribute containers worker node.
Worker node will maintain containers.    
port : 2377
Worker nodes join


SETUP:
1. CREATE 3 SERVERS AND INSTALL DOCKER
2. SET HOSTNAMES (hostnamectl set-hostname manager/worker1/worker2)
3. GO TO MANAGER NODE (docker swarm init) -- > copy token to all nodes
4. docker node ls


Install docker in master, node1 and node2

yum install -y docker

systemctl start docker


--> docker swarm init  -- [ In manager node ]

It will generate a token and command, run below command in all worker nodes

In Node1
--------
docker swarm join --token SWMTKN-1-5knz8d9ypaqvd6s2sm1mlyy8d9bznuwemwm18dbz5p7jfxplad-b35y8xpsgq2xgza04913lv91z 172.31.27.158:2377

In Node2
--------
docker swarm join --token SWMTKN-1-5knz8d9ypaqvd6s2sm1mlyy8d9bznuwemwm18dbz5p7jfxplad-b35y8xpsgq2xgza04913lv91z 172.31.27.158:2377

Now create a container in manager node, it should create in all worker nodes

On Manager
----------

docker node ls    

-- docker run -itd --name contl -p 81:80 trainerreyaz/ib-image     [use dockerhub image]

But the above command will create a container only in manager as we used just docker command. If we want to have container in all nodes use docker service



-- docker service create --name internetbanking --replicas 3 -p 81:80 trainerreyaz/ib-image:latest

-- docker ps -a

Now see containers in all worker nodes, go to each workernodes and do ps -a

docker ps -a

Access application on all 3 servers http://IP:81  , http://workernodeip:81


-- docker service ls

-- docker service ps internetbanking

-- docker service logs internetbanking

-- docker service inspect internetbanking

-- docker service scale internetbanking=10    --- it will scale out , equally distributed to all worker nodes including manager

-- docker service scale internetbanking=3  -- it will scale down, it uses LIFO,

give example again
docker service scale internetbanking=15
docker service scale internetbanking=5

-- docker service rollback internetbanking  --- going back to how many containers was there before
it will show 15
again rollback it will show 5

scenario: if we delete the container in worker node docker kill containerid , docker rm containerid and check the http://IP:81 it still work
because, nodes has self healing , if you do docker ps -a  , you can see a new container created automatically

delete the container again, it will re-create it automatically
docker stop contids
docker rm contids

docker ps -a

--> now in manager node delete the service which contain containers

-- docker service ls

-- docker service rm internetbanking

-- docker service ls

if you want to re-create  

docker service create --name loan --replicas 3 -p 82:80 trainerreyaz/loan:latest

docker service ps loan

docker service create --name mobilebanking --replicas 3 -p 83:80 trainerreyaz/mb-image:latest


********** No auto-scaling and load balancer in DockerSwarm that why we use Kubernetes

Note: If entire swarm is not working
docker swarm init --force-new-cluster

===================================================================
Example:

docker service create --name jenkins --replicas 3 -p 8080:8080 jenkins/jenkins:lts   ---> create a new service called Jenkins and setup Jenkins containers in all nodes

jenkins/jenkins:lts -> username/image:tag


=============================================================================================================================================

Cluster Activities
=================

docker swarm leave  --> this will leave the swarm, do this in worker node
if you want to join back -> use the same join command to join back to swarm (up arrow or history command) or

docker swarm join-token manager --> this command will generate a token to join
docker swarm join-token worker --> this command will generate a token for worker nodes to join

docker node ls

docker node rm nodeid --> it will remove the down node

===================
Docker Networking
===================

Docker networking are used to make communication between the multiple containers that are running on same or different docker hosts

Different Docker Networks
 --> Bridge Network = Same hosts (if 2 containers wants to communicate with each other in same docker host)
 --> Overlay Network = different hosts (2 containers want to communicate with each other in different docker hosts)
 --> Host Network = if you want to use HOST meaning EC2 networking
 --> None Network = if you dont want to expose any application


-- docker node ls

-- docker network ls

Explain Name and Driver as per output

-- docker network create reyaz

-- docker network ls


-- docker run -itd --name cont1 -p 81:80 nginx:latest
-- docker run -itd --name cont2 -p 82:80 nginx:latest

-- docker inspect cont1
-- docker inspect cont2

BY default networking part it say Bridge by default

Lets change the network for cont1 to Reyaz network that we created before

-- docker network connect reyaz cont1  
-- docker network connect reyaz cont2

docker network inspect Reyaz  --> this will show cont1 and cont2 in same network

Lets see if 2 containers can communicate each other or not

Get the IP address of cont1
 
 -- docker inspect cont1


first connect to cont2

-- docker exec -it cont2 /bin/bash

now you are in cont2

ping IP of cont1 --> ping will not work

apt update
apt install iputils-ping -y
ping cont1ip

ctrl p q

docker network disconnect Reyaz cont2 --> if you want to remove cont2 from Reyaz network

docker network inspect Reyaz  --> now you cannot see cont2 in that json

docker network prune --> this will delete unused networks

docker system prune --> also delete the networks

=========================
Docker Python FLask Project
=========================

yum install -y docker

yum install -y git

git clone https://github.com/ReyazShaik/docker-python-flask-project.git

cd docker-python-flask-project

Show files

vi Dockerfile

FROM python:3.6
COPY . /app
WORKDIR /app
RUN pip install -r requirements.txt
EXPOSE 5000
CMD ["python", "app.py"]

Create image
-----------

docker build -t pythonapp .

Create Container
----------------
docker run -d -p 5000:5000 pythonapp


http://ip:5000

======================================================================================================================================

====================================================================

DOCKER - PROJECT - 

Manager  - t2.micro
WorkerNode1 -micro
WorkerNode2 - micro



1. Launch 3 Amazon Linux 3 instances 1. DockerSwarm 2. Worker Node 1 , 3. Worker Node 2
2. Open MobaXterm and connect to all machines
3. Go to multi-exec and install docker and start
   yum install -y docker
   systemctl start docker

set hostnames for all

hostnamectl set-hostname manager  --> In manager node
sudo -i
hostnamectl set-hostname worker1
sudo -i
hostnamectl set-hostname worker2
sudo -i

In Manager Node

docker swarm init

IN worker node1

docker swarm join --token SWMTKN-1-52jo6saicq2fg67gicpy2t77t1xjhaeqb41520jfux4y77rib7-ah49urfxgvcgmdg09yehhkbbd 172.31.21.171:2377

In Worker Node2

docker swarm join --token SWMTKN-1-52jo6saicq2fg67gicpy2t77t1xjhaeqb41520jfux4y77rib7-ah49urfxgvcgmdg09yehhkbbd 172.31.21.171:2377

In Manager node

docker node ls

--> install Jenkins on master node
vi Jenkins.sh
copy paste the script

sh Jenkins.sh -- selection 2

--> if doesn't work see if Jenkins containers is already there as we did docker swarm -- for that docker service rm Jenkins

http://IP:8080 and ready Jenkins

Run the below command to work docker in Jenkins pipeline otherwise pipeline will not work
-----------------------

RUn in docker host - manager

chmod 777 /var/run/docker.sock
systemctl daemon-reload
systemctl restart docker.service


Now lets create a pipeline in Jenkins
-------------------------------------

Get code from GitHub --> as dockerfile exits in the repo, we can build using pipeline
But we dont have only 1 image, we need to create multiple images like internetbanking, Mobilebanking, loan and insurance
For this, lets have image as a variable not a direct entry in pipeline as docker build -t internetbanking:v1 , instead use docker build -t $image

create image as a parameter

In Pipeline select : This project is parameterized --> Add Parameter --> choice parameter
Name:  image
choices :
internetbanking:v1
mobilebanking:v1
insurance:v1
loans:v1


pipeline {
    agent any
   
    stages {
        stage('checkout') {
            steps {
                git 'https://github.com/ReyazShaik/dockerproject.git&#39;
            }
        }
        stage('build') {
            steps {
                sh 'docker build -t $image .'
            }
        }
     }
}



Run the Pipeline

in Docker host / manager , see image got created

docker images --> you should see internetbanking:v1 image

----
Now lets tag
----

As we need to create multiple repo and images, we need to create a parameter for repo , go up in same pipeline --> Add parameters

Name: repo  -- keep always small letters because in pipeline also $repo
choices :
trainerreyaz/ib-image
trainerreyaz/mb-image
trainerreyaz/insurance-image
trainerreyaz/loans-image


IN pipeline add

 stage('tagging') {
            steps {
                sh 'docker tag $image $repo'
            }
        }



Now in docker host --> docker images -> you can see now new image with new tag

Now images are stored locally , now push to dockerhub , for this we need to authenticate to dockerhub

In Jenkins, we have local and global variables

Manage Jenkins --> System --> Environment variables
Name: password
value: give here dockerhub password


        stage('push') {
            steps {
                sh 'docker login -u trainerreyaz -p $password'
                sh 'docker push $repo'
            }
        }

Now build the pipeline with internetbanking --> image will be stored in dockerhub --> login to dockerhub and show the image

Now, change index.html in GitHub from InternetBanking to MobileBanking --> Build Pipeline --> MobileBanking --> trainerreyaz/mb-image
Now, change index.html in GitHub from MobileBanking to Insurance --> Build Pipeline --> Insurance --> trainerreyaz/insurance-image
Now, change index.html in GitHub from Insurance to Loans --> Build Pipeline --> Loans --> trainerreyaz/Loans-image

Now a new repos and image will be created in dockerhub

Note: In realtime we dont use webhook because if developer push automatically pipeline will run which is danger

Till now we created images lets now do deployment

=== NOw Deployment ===

Lets use docker swarm to have HA , for this we need containers, lets use dockercompose
And docker compose is use to create multiple containers but in single host but we need containers in all hosts(workernodes)
Lets use docker stack

show docker-compose.yml in GitHub

version: '3.8'
services:
  internetbanking:
    image: trainerreyaz/ib-image:latest
    ports:
      - "81:80"
    deploy:
      replicas: 3
    volumes:
      - /var/lib/internetbanking
  mobilebanking:
    image: trainerreyaz/mb-image:latest
    ports:
      - "82:80"
    deploy:
      replicas: 3
    volumes:
      - /var/lib/mobilebanking
  insurance:
    image: trainerreyaz/insurance-image:latest
    ports:
      - "83:80"
    deploy:
      replicas: 3
    volumes:
      - /var/lib/insurance
  loan:
    image: trainerreyaz/loan-image:latest
    ports:
      - "84:80"
    deploy:
      replicas: 3
    volumes:
      - /var/lib/loan



And add this below to pipeline , and bank is application name

 stage('deploy') {
            steps {
                sh 'docker stack deploy -c docker-compose.yml bank'
            }
        }

-C - compose file

Now build the pipeline with insurance image and insurance repo

Some docker stack commands
=====================

docker service ls
docker service rm stackname
docker stack ls
docker stack ps bank


pipeline {
    agent any
   
    stages {
        stage('checkout') {
            steps {
                git 'https://github.com/ReyazShaik/dockerproject.git&#39;
            }
        }
        stage('build') {
            steps {
                sh 'docker build -t $image .'
            }
        }
        stage('tag') {
            steps {
                sh 'docker tag $image $repo'
            }
        }
        stage('push') {
            steps {
                sh 'docker login -u trainerreyaz -p $password'
                sh 'docker push $repo'
            }
        }
        stage('deploy') {
            steps {
                sh 'docker stack deploy bank -c docker-compose.yml'
            }
        }
    }
}



PORTAINER - GUI for managing containers
=======================================


Portainer Using SWARM
===========

Must have swarm mode and all ports enable with docker engine

In Manager Node
----------------

docker swarm init

IN worker node1
----------------

docker swarm join --token SWMTKN-1-52jo6saicq2fg67gicpy2t77t1xjhaeqb41520jfux4y77rib7-ah49urfxgvcgmdg09yehhkbbd 172.31.21.171:2377

In Worker Node2
---------------

docker swarm join --token SWMTKN-1-52jo6saicq2fg67gicpy2t77t1xjhaeqb41520jfux4y77rib7-ah49urfxgvcgmdg09yehhkbbd 172.31.21.171:2377

In Manager node
---------------

docker node ls


Download portainer yaml
----------------------

curl -L https://downloads.portainer.io/ce2-16/portainer-agent-stack.yml -o portainer-agent-stack.yml

docker stack deploy -c portainer-agent-stack.yml portainer

docker ps

public-ip of swarm master:9443

https://13.233.198.125:9443
http://13.233.198.125:900

Environment --> Primary


Portainer on Standalone docker
-------------------------------



docker volume create portainer_data

docker run -d -p 8000:8000 -p 9443:9443 --name portainer \
--restart=always \
-v /var/run/docker.sock:/var/run/docker.sock \
-v portainer_data:/data \
portainer/portainer-ce:2.9.3

docker ps

https://localhost:9443   [use https forcely in browser]

Following command to start a Portainer Agent container on the system you want to add:

docker run -d -p 9001:9001 --name portainer_agent --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v /var/lib/docker/volumes:/var/lib/docker/volumes portainer/agent:2.9.3


Add environment --> Name = Docker, Environment URL = EC2 instance Public IP:9001

Local --> Add container --> Name = Apache --> image= httpd,  publish a new network port --> host=8080, container =80 --> deploy


explore application

https://phoenixnap.com/kb/docker-portainer-install





TO setup MySQL
==============

docker run -d --name mysql-db \
  -e MYSQL_ROOT_PASSWORD=my-secret-pw \
  -e MYSQL_DATABASE=mydb \
  -e MYSQL_USER=myuser \
  -e MYSQL_PASSWORD=mypassword \
  -p 3306:3306 \
  -v mysql_data:/var/lib/mysql \
  --restart unless-stopped \
  mysql:latest


docker ps


docker logs mysql-db

docker exec -it mysql-db mysql -u root -p

pwd= my-secret-pw


create database test123;
show databases;
drop databases test123;